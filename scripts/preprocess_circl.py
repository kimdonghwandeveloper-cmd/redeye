"""
CIRCL/vulnerability-cwe-patch ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” CIRCL ë°ì´í„°ì…‹ì˜ íŒ¨ì¹˜ diffë¥¼ íŒŒì‹±í•˜ì—¬:
1. Detection Modelìš©: ì·¨ì•½/ì•ˆì „ ì½”ë“œ ìŠ¤ë‹ˆí« + ë¼ë²¨ ë°ì´í„° ìƒì„±
2. Repair Modelìš©: ì·¨ì•½ ì½”ë“œ â†’ ìˆ˜ì • ì½”ë“œ ìŒ ìƒì„±

v2: ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ JSONLë¡œ ìŠ¤íŠ¸ë¦¬ë° ì €ì¥ + ìƒ˜í”Œë§
"""

import base64
import json
import os
import re
import random
from collections import Counter

# Configuration
OUTPUT_DIR = "./data/circl_processed"
MAX_CONTEXT_LINES = 5    # diff context ë¼ì¸ ìˆ˜
MIN_CODE_LENGTH = 20     # ë„ˆë¬´ ì§§ì€ ì½”ë“œ ì œì™¸
MAX_CODE_LENGTH = 2000   # ë„ˆë¬´ ê¸´ ì½”ë“œ ì œì™¸ (í† í° ì´ˆê³¼ ë°©ì§€)
MAX_DETECTION_SAMPLES = 50000  # Detection ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ë°¸ëŸ°ì‹±)
MAX_REPAIR_SAMPLES = 20000    # Repair ìµœëŒ€ ìƒ˜í”Œ ìˆ˜

# ì§€ì› ì–¸ì–´ í™•ì¥ì ë§¤í•‘
LANG_EXTENSIONS = {
    ".py": "python", ".js": "javascript", ".ts": "typescript",
    ".java": "java", ".cs": "csharp", ".cpp": "cpp", ".c": "c",
    ".go": "go", ".rs": "rust", ".swift": "swift", ".kt": "kotlin",
    ".php": "php", ".rb": "ruby", ".sql": "sql"
}


def detect_language(patch_url: str, diff_text: str) -> str:
    """íŒ¨ì¹˜ URL ë˜ëŠ” diff íŒŒì¼ëª…ì—ì„œ ì–¸ì–´ ì¶”ë¡ ."""
    # diff í—¤ë”ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ: --- a/path/to/file.py
    file_match = re.findall(r'--- a/(.+)', diff_text)
    if file_match:
        filename = file_match[0].strip()
        ext = os.path.splitext(filename)[1].lower()
        if ext in LANG_EXTENSIONS:
            return LANG_EXTENSIONS[ext]
    
    # URLì—ì„œ ì¶”ë¡ 
    for ext, lang in LANG_EXTENSIONS.items():
        if ext in patch_url.lower():
            return lang
    
    return "unknown"


def parse_diff_for_detection(diff_text: str, language: str) -> list:
    """
    diffì—ì„œ Detection Modelìš© ë°ì´í„° ì¶”ì¶œ.
    
    - ë¼ì¸(ì·¨ì•½ ì½”ë“œ): label = 1 (VULNERABLE)
    + ë¼ì¸(ìˆ˜ì • ì½”ë“œ): label = 0 (SAFE)
    
    ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ë„ í¬í•¨í•˜ì—¬ ë” ì •í™•í•œ í•™ìŠµ ë°ì´í„° ìƒì„±.
    """
    samples = []
    current_hunk_context = []
    vuln_lines = []
    safe_lines = []
    
    for line in diff_text.split('\n'):
        if line.startswith('@@'):
            # ì´ì „ hunk ì²˜ë¦¬
            if vuln_lines or safe_lines:
                context = '\n'.join(current_hunk_context[-MAX_CONTEXT_LINES:])
                
                if vuln_lines:
                    vuln_code = '\n'.join(vuln_lines)
                    if MIN_CODE_LENGTH <= len(vuln_code) <= MAX_CODE_LENGTH:
                        full_code = f"{context}\n{vuln_code}" if context else vuln_code
                        samples.append({
                            "code": full_code.strip(),
                            "label": 1,
                            "language": language
                        })
                
                if safe_lines:
                    safe_code = '\n'.join(safe_lines)
                    if MIN_CODE_LENGTH <= len(safe_code) <= MAX_CODE_LENGTH:
                        full_code = f"{context}\n{safe_code}" if context else safe_code
                        samples.append({
                            "code": full_code.strip(),
                            "label": 0,
                            "language": language
                        })
            
            current_hunk_context = []
            vuln_lines = []
            safe_lines = []
            
        elif line.startswith('---') or line.startswith('+++'):
            continue
        elif line.startswith('-'):
            vuln_lines.append(line[1:])
        elif line.startswith('+'):
            safe_lines.append(line[1:])
        else:
            # Context line (unchanged)
            current_hunk_context.append(line.lstrip(' '))
    
    # ë§ˆì§€ë§‰ hunk ì²˜ë¦¬
    if vuln_lines or safe_lines:
        context = '\n'.join(current_hunk_context[-MAX_CONTEXT_LINES:])
        if vuln_lines:
            vuln_code = '\n'.join(vuln_lines)
            if MIN_CODE_LENGTH <= len(vuln_code) <= MAX_CODE_LENGTH:
                full_code = f"{context}\n{vuln_code}" if context else vuln_code
                samples.append({"code": full_code.strip(), "label": 1, "language": language})
        if safe_lines:
            safe_code = '\n'.join(safe_lines)
            if MIN_CODE_LENGTH <= len(safe_code) <= MAX_CODE_LENGTH:
                full_code = f"{context}\n{safe_code}" if context else safe_code
                samples.append({"code": full_code.strip(), "label": 0, "language": language})
    
    return samples


def parse_diff_for_repair(diff_text: str, language: str) -> list:
    """
    diffì—ì„œ Repair Modelìš© ë°ì´í„° ì¶”ì¶œ.
    ì·¨ì•½ ì½”ë“œ(- ë¼ì¸) â†’ ìˆ˜ì • ì½”ë“œ(+ ë¼ì¸) ìŒ ìƒì„±.
    """
    pairs = []
    vuln_lines = []
    safe_lines = []
    
    for line in diff_text.split('\n'):
        if line.startswith('@@'):
            # ì´ì „ hunkì—ì„œ ìŒ ìƒì„±
            if vuln_lines and safe_lines:
                vuln_code = '\n'.join(vuln_lines)
                safe_code = '\n'.join(safe_lines)
                if MIN_CODE_LENGTH <= len(vuln_code) <= MAX_CODE_LENGTH and \
                   MIN_CODE_LENGTH <= len(safe_code) <= MAX_CODE_LENGTH:
                    pairs.append({
                        "input": f"fix vulnerability: {vuln_code}",
                        "output": safe_code,
                        "language": language
                    })
            vuln_lines = []
            safe_lines = []
        elif line.startswith('---') or line.startswith('+++'):
            continue
        elif line.startswith('-'):
            vuln_lines.append(line[1:])
        elif line.startswith('+'):
            safe_lines.append(line[1:])
    
    # ë§ˆì§€ë§‰ hunk
    if vuln_lines and safe_lines:
        vuln_code = '\n'.join(vuln_lines)
        safe_code = '\n'.join(safe_lines)
        if MIN_CODE_LENGTH <= len(vuln_code) <= MAX_CODE_LENGTH and \
           MIN_CODE_LENGTH <= len(safe_code) <= MAX_CODE_LENGTH:
            pairs.append({
                "input": f"fix vulnerability: {vuln_code}",
                "output": safe_code,
                "language": language
            })
    
    return pairs


def save_jsonl(data: list, filepath: str):
    """JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )."""
    with open(filepath, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    size_mb = os.path.getsize(filepath) / (1024 * 1024)
    print(f"  ğŸ’¾ Saved {len(data)} samples â†’ {filepath} ({size_mb:.1f} MB)")


def balance_detection_samples(samples: list, max_total: int) -> list:
    """Detection ìƒ˜í”Œì„ ë¼ë²¨ ë°¸ëŸ°ì‹±í•˜ì—¬ ìƒ˜í”Œë§."""
    vuln = [s for s in samples if s["label"] == 1]
    safe = [s for s in samples if s["label"] == 0]
    
    half = max_total // 2
    random.seed(42)
    
    if len(vuln) > half:
        vuln = random.sample(vuln, half)
    if len(safe) > half:
        safe = random.sample(safe, half)
    
    # ì ì€ ìª½ì— ë§ì¶”ê¸°
    min_count = min(len(vuln), len(safe))
    vuln = vuln[:min_count]
    safe = safe[:min_count]
    
    combined = vuln + safe
    random.shuffle(combined)
    return combined


def main():
    from datasets import load_dataset
    
    print("ğŸ“¥ Loading CIRCL/vulnerability-cwe-patch dataset...")
    dataset = load_dataset("CIRCL/vulnerability-cwe-patch", split="train")
    print(f"âœ… Loaded {len(dataset)} entries")
    
    all_detection = []
    all_repair = []
    lang_counter = Counter()
    skipped = 0
    
    for i, entry in enumerate(dataset):
        if i % 5000 == 0:
            print(f"  Processing {i}/{len(dataset)}... (det={len(all_detection)}, rep={len(all_repair)})")
        
        patches = entry.get("patches", [])
        if not patches:
            skipped += 1
            continue
        
        for patch in patches:
            patch_b64 = patch.get("patch_text_b64", "")
            patch_url = patch.get("url", "")
            
            if not patch_b64:
                continue
            
            try:
                diff_text = base64.b64decode(patch_b64).decode("utf-8", errors="replace")
            except Exception:
                continue
            
            language = detect_language(patch_url, diff_text)
            
            # TOP 10 ì–¸ì–´ë§Œ í•„í„°ë§
            if language == "unknown":
                continue
            
            lang_counter[language] += 1
            
            # Detectionìš©
            det_samples = parse_diff_for_detection(diff_text, language)
            all_detection.extend(det_samples)
            
            # Repairìš©
            rep_samples = parse_diff_for_repair(diff_text, language)
            all_repair.extend(rep_samples)
    
    # ê²°ê³¼ í†µê³„ (ìƒ˜í”Œë§ ì „)
    print(f"\nğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼ (ìƒ˜í”Œë§ ì „):")
    print(f"  - ì „ì²´ ì—”íŠ¸ë¦¬: {len(dataset)}")
    print(f"  - ìŠ¤í‚µë¨ (íŒ¨ì¹˜ ì—†ìŒ): {skipped}")
    print(f"  - Detection ìƒ˜í”Œ: {len(all_detection)}")
    print(f"  - Repair ìƒ˜í”Œ: {len(all_repair)}")
    print(f"\nğŸŒ ì–¸ì–´ë³„ ë¶„í¬:")
    for lang, count in lang_counter.most_common():
        print(f"  {lang}: {count}")
    
    # ìƒ˜í”Œë§ (ë©”ëª¨ë¦¬ ì ˆì•½ + í•™ìŠµ íš¨ìœ¨)
    print(f"\nâœ‚ï¸ ìƒ˜í”Œë§ ì‹œì‘...")
    det_sampled = balance_detection_samples(all_detection, MAX_DETECTION_SAMPLES)
    print(f"  Detection: {len(all_detection)} â†’ {len(det_sampled)} (ë°¸ëŸ°ì‹±ë¨)")
    
    random.seed(42)
    if len(all_repair) > MAX_REPAIR_SAMPLES:
        rep_sampled = random.sample(all_repair, MAX_REPAIR_SAMPLES)
    else:
        rep_sampled = all_repair
    print(f"  Repair: {len(all_repair)} â†’ {len(rep_sampled)}")
    
    # JSONLë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì !)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    det_path = os.path.join(OUTPUT_DIR, "detection.jsonl")
    save_jsonl(det_sampled, det_path)
    
    rep_path = os.path.join(OUTPUT_DIR, "repair.jsonl")
    save_jsonl(rep_sampled, rep_path)
    
    # Label ë¶„í¬ í™•ì¸
    det_labels = [s["label"] for s in det_sampled]
    det_langs = Counter(s["language"] for s in det_sampled)
    rep_langs = Counter(s["language"] for s in rep_sampled)
    
    print(f"\nğŸ·ï¸ Detection Label ë¶„í¬: SAFE={det_labels.count(0)}, VULNERABLE={det_labels.count(1)}")
    print(f"\nğŸŒ Detection ì–¸ì–´ë³„:")
    for lang, count in det_langs.most_common():
        print(f"  {lang}: {count}")
    print(f"\nğŸŒ Repair ì–¸ì–´ë³„:")
    for lang, count in rep_langs.most_common():
        print(f"  {lang}: {count}")
    
    print("\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
